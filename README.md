# Vision Transformer

![nVIDIA](https://img.shields.io/badge/cuda-green.svg?style=for-the-badge&logo=nVIDIA&logoColor=white)
![C++](https://img.shields.io/badge/c++-%2300599C.svg?style=for-the-badge&logo=c%2B%2B&logoColor=white)
![CMake](https://img.shields.io/badge/CMake-%23008FBA.svg?style=for-the-badge&logo=cmake&logoColor=white)

> **ðŸ’¡ Puedes ejecutar el entrenamiento en GPU con CUDA desde [Google Colab](https://colab.research.google.com/drive/1n7AIsbDGGtv3eTVO681Enz35pO3e5fWF?usp=sharing):**  
> [![Abrir en Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1n7AIsbDGGtv3eTVO681Enz35pO3e5fWF?usp=sharing)

---

## Instrucciones para ejecutar el proyecto con GPU (CUDA)

### **_Requisitos:_**

| **Herramienta** | **Usado**       | **Recomendado** |
| --------------- | --------------- | --------------- |
| **CMake**       | 3.30.5          | â‰¥ 3.26          |
| **G++ (GCC)**   | 13.3.1          | < 13.x          |
| **CUDA (nvcc)** | 12.6 (V12.6.85) | 12.0 o superior |


### 1. Clonar repositorio, Descargar y descomprimir los datos

```bash
git clone https://github.com/PaulParizacaMozo/VisionTransformer.git
cd VisionTransformer
cd cuda_Vit
```

El proyecto utiliza un archivo comprimido llamado `data.zip` que contiene los conjuntos de datos necesarios para la ejecuciÃ³n (Fashion MNIST y MNIST).

1. Descarga el archivo `data.zip` desde el repositorio.
2. Descomprime el archivo `data.zip` en el directorio raÃ­z de tu proyecto. Esto crearÃ¡ un directorio `data` que contiene los siguientes archivos CSV:
   - `fashion_test.csv`
   - `fashion_train.csv`
   - `mnist_test.csv`
   - `mnist_train.csv`

```bash
wget https://github.com/PaulParizacaMozo/VisionTransformer/releases/download/data/data.zip
7z x data.zip
```

### 2. Preparar el script `run.sh`

Antes de ejecutar el script, asegÃºrate de darle permisos de ejecuciÃ³n:

```bash
chmod +x run.sh
```

### 3. Ejecutar el proyecto

- Para ejecutar el entrenamiento:

```bash
./run.sh
```

- Para testear el modelo en el conjunto de pruebas:

```bash
./run.sh test
```

- Para testear el modelo con una imagen aleatoria `.png`:

```bash
./run.sh image data/7.png
```


## Instrucciones para ejecutar el proyecto con CPU

### 1. Clonar repositorio, Descargar y descomprimir los datos

```bash
git clone https://github.com/PaulParizacaMozo/VisionTransformer.git
cd VisionTransformer
```

El proyecto utiliza un archivo comprimido llamado `data.zip` que contiene los conjuntos de datos necesarios para la ejecuciÃ³n (Fashion MNIST y MNIST).

1. Descarga el archivo `data.zip` desde el repositorio.
2. Descomprime el archivo `data.zip` en el directorio raÃ­z de tu proyecto. Esto crearÃ¡ un directorio `data` que contiene los siguientes archivos CSV:
   - `fashion_test.csv`
   - `fashion_train.csv`
   - `mnist_test.csv`
   - `mnist_train.csv`

```bash
wget https://github.com/PaulParizacaMozo/VisionTransformer/releases/download/data/data.zip
7z x data.zip
```

### 2. Preparar el script `run.sh`

Antes de ejecutar el script, asegÃºrate de darle permisos de ejecuciÃ³n:

```bash
chmod +x run.sh
```

### 3. Ejecutar el proyecto

- Para ejecutar el entrenamiento:

```bash
./run.sh
```

- Para testear el modelo en el conjunto de pruebas:

```bash
./run.sh test
```

- Para testear el modelo con una imagen aleatoria `.png`:

```bash
./run.sh image data/7.png
```

- Para ejecutar el visualizador en tiempo real:

```bash
./run.sh visualizer
```

Este script compilarÃ¡ y ejecutarÃ¡ el proyecto. Los datos de entrada se leerÃ¡n desde el directorio `data`.

---

## Resultados

### Dataset: MNIST

Este experimento entrena un modelo Vision Transformer (ViT) en el dataset **MNIST** usando los siguientes hiperparÃ¡metros:

#### **1. HiperparÃ¡metros del Modelo**

| ParÃ¡metro        | Valor | Comentario                                   |
| ---------------- | ----- | -------------------------------------------- |
| `embedding_dim`  | 96    | DimensiÃ³n de embedding (recomendado: 96â€“196) |
| `num_layers`     | 8     | Capas Transformer Encoder                    |
| `num_heads`      | 8     | Cabezas de atenciÃ³n multihead                |
| `patch_size`     | 14    | TamaÃ±o del parche (patch)                    |
| `num_classes`    | 10    | MNIST tiene 10 clases (0â€“9)                  |
| `in_channels`    | 1     | Canal Ãºnico (imÃ¡genes en escala de grises)   |
| `mlp_hidden_dim` | 384   | `embedding_dim * 4`                          |
| `dropout_rate`   | 0.2   | RegularizaciÃ³n                               |

#### **2. HiperparÃ¡metros de Entrenamiento**

| ParÃ¡metro       | Valor | Comentario                                 |
| --------------- | ----- | ------------------------------------------ |
| `epochs`        | 50    | NÃºmero total de Ã©pocas                     |
| `batch_size`    | 128   | TamaÃ±o de batch                            |
| `learning_rate` | 3e-4  | Tasa de aprendizaje inicial                |
| `weight_decay`  | 1e-4  | RegularizaciÃ³n L2                          |
| `lr_init`       | 3e-4  | Tasa inicial de LR (igual a learning_rate) |
| `warmup_frac`   | 0.1   | 10% de las Ã©pocas serÃ¡n warm-up            |

#### **3. ConfiguraciÃ³n de Datos**

| ParÃ¡metro         | Valor          | Comentario                        |
| ----------------- | -------------- | --------------------------------- |
| `sample_frac`     | 1.0            | Se usa el 100% del dataset MNIST  |
| `train_frac`      | 0.80           | 80% para entrenamiento            |
| `val_frac`        | 0.20           | 20% para validaciÃ³n               |
| `num_channels`    | 1              | Escala de grises                  |
| `height`, `width` | 28 Ã— 28        | TamaÃ±o original de imagen         |
| `mean`, `std`     | 0.1307, 0.3081 | NormalizaciÃ³n estÃ¡ndar para MNIST |

#### Ejecucion

- **Entrenamiento:**

  ![](cuda_Vit/.docs/mnist_train.png)

- **Test:**

  ![](cuda_Vit/.docs/mnist_test.png)

#### Modelo Preentrenado

El modelo ya ha sido entrenado y se encuentra guardado en:

```
models/vit_mnist_test
```

Para probarlo simplemente ejecuta:

```bash
./run.sh test
```

---

## Arquitectura del Modelo

Este proyecto implementa una arquitectura Vision Transformer (ViT) desde cero en C++, diseÃ±ada para clasificaciÃ³n de imÃ¡genes en conjuntos de datos como MNIST, FashionMNIST y BloodMNIST. La implementaciÃ³n sigue los principios fundamentales de los Transformers adaptados a datos de imagen.

### Diagrama de la Arquitectura

![Vision Transformer Architecture Diagram](.docs/IA_VIT_DIAGRAM.png)

### Componentes Principales

#### 1. Preprocesamiento de Entrada (Imagen)

- **Entrada**: ImÃ¡genes 28x28 pÃ­xeles (1 canal para MNIST)
- **DivisiÃ³n en parches**:
  - TamaÃ±o de parche: 14x14
  - Parches generados: 4 (28/14 Ã— 28/14)
- **Token [CLS]**:
  - Vector especial aÃ±adido al inicio de la secuencia
  - DimensiÃ³n: 64 (igual que los embeddings)
- **ProyecciÃ³n lineal**:
  - Cada parche (196 pÃ­xeles) â†’ vector de 64 dimensiones

#### 2. Input Embedding

- **Embeddings de parches**:
  - ProyecciÃ³n lineal de 196 â†’ 64 dimensiones
- **Token [CLS]**:
  - Vector aprendido de 64 dimensiones
- **Secuencia de salida**:
  - Formato: `[batch_size, 5, 64]` (1 token CLS + 4 parches)

#### 3. Positional Encoding

- **CodificaciÃ³n posicional**:
  - Embeddings aprendidos para cada posiciÃ³n
  - 5 posiciones: [CLS] + 4 parches
- **Suma con embeddings**:
  - `Embeddings + Positional Encodings`
  - Salida: `[batch_size, 5, 64]`

#### 4. Bloques Encoder (8 capas)

Cada bloque contiene:

##### 3.A. Multi-Head Attention (16 cabezas)

- **Proyecciones lineales**:
  - Entrada dividida en 16 cabezas (dimensiÃ³n por cabeza = 4)
  - `Q_i = X @ W_i^Q`, `K_i = X @ W_i^K`, `V_i = X @ W_i^V`
- **Scaled Dot-Product Attention**:
  - `Scores = (Q_i @ K_i^T) / âˆšd_k`
  - Softmax â†’ AtenciÃ³n ponderada
- **ConcatenaciÃ³n y proyecciÃ³n**:
  - `Concat(Head_1, ..., Head_16)`
  - `Output_MHA = Concat @ W_O`

##### 3.B. Add & Norm

- ConexiÃ³n residual + Layer Normalization
- `LayerNorm(X + Output_MHA)`

##### 3.C. Position-wise FFN

- **MLP de 2 capas**:
  1. `Linear(64 â†’ 256)` + activaciÃ³n GELU
  2. `Linear(256 â†’ 64)`
- Dropout (20%)

##### 3.D. Add & Norm

- ConexiÃ³n residual + Layer Normalization
- `LayerNorm(Input_FFN + Output_FFN)`

#### 5. Salida y ClasificaciÃ³n

- **ExtracciÃ³n del token [CLS]**:
  - Primera posiciÃ³n de la secuencia (`Ã­ndice=0`)
  - Vector de 64 dimensiones
- **Clasificador lineal**:
  - `Linear(64 â†’ 10)` (10 clases para MNIST)
- **Softmax**:
  - DistribuciÃ³n de probabilidad sobre las clases

## HiperparÃ¡metros Configurables

| ParÃ¡metro        | Valor | DescripciÃ³n                    |
| ---------------- | ----- | ------------------------------ |
| `embedding_dim`  | 64    | DimensiÃ³n de los embeddings    |
| `num_layers`     | 8     | NÃºmero de bloques encoder      |
| `num_heads`      | 16    | Cabezas de atenciÃ³n multi-head |
| `patch_size`     | 14    | TamaÃ±o de los parches (14Ã—14)  |
| `mlp_hidden_dim` | 256   | DimensiÃ³n oculta en FFN        |
| `dropout_rate`   | 0.2   | Tasa de dropout                |
| `batch_size`     | 64    | TamaÃ±o del lote                |
| `learning_rate`  | 3e-4  | Tasa de aprendizaje            |
| `weight_decay`   | 1e-4  | Decaimiento de pesos           |

## CompilaciÃ³n y EjecuciÃ³n

### ConfiguraciÃ³n de datos

1. Descargar conjuntos de datos MNIST en formato CSV
2. Colocar en directorio `data/`:
   - `mnist_train.csv`
   - `mnist_test.csv`

## Flujo de Entrenamiento

1. **Carga de datos**:

   - Lectura de imÃ¡genes y etiquetas desde CSV
   - NormalizaciÃ³n: media=0.1307, desviaciÃ³n=0.3081

2. **Preprocesamiento**:

   - DivisiÃ³n en parches
   - CreaciÃ³n de tokens [CLS]
   - ProyecciÃ³n lineal a embeddings

3. **Entrenamiento**:

   - Forward pass a travÃ©s de los 8 bloques encoder
   - ExtracciÃ³n del token [CLS]
   - ClasificaciÃ³n lineal + softmax
   - CÃ¡lculo de pÃ©rdida (cross-entropy)
   - Backpropagation y optimizaciÃ³n (AdamW)

4. **EvaluaciÃ³n**:

   - ValidaciÃ³n despuÃ©s de cada Ã©poca
   - CÃ¡lculo de precisiÃ³n en conjunto de validaciÃ³n

5. **Guardado del modelo**:
   - Pesos en formato binario (`vit_mnist.weights`)
   - ConfiguraciÃ³n en JSON (`vit_mnist.json`)

## PersonalizaciÃ³n

Para usar con otros conjuntos de datos:

```cpp
ViTConfig model_config;
model_config.embedding_dim = 128;   // Aumentar dimensiÃ³n
model_config.num_classes = 10;      // Para datasets con mÃ¡s clases
model_config.in_channels = 3;        // Para imÃ¡genes RGB (BloodMNIST)
```

## Resultados Esperados

- **MNIST**:

  - Accuracy >90% en alrededor de 10 Ã©pocas

- **FashionMNIST**:

  - Accuracy >85% en alrededor de 20 Ã©pocas

- **BloodMNIST** (imÃ¡genes 3x28x28):
  - Accuracy ~75-80% en alrededor de 30 Ã©pocas



## Estructura del Proyecto

A continuaciÃ³n se muestra la estructura de directorios del proyecto:

```
.
â”œâ”€â”€ app
â”‚   â””â”€â”€ main.cpp
â”œâ”€â”€ CMakeLists.txt
â”œâ”€â”€ compile\_commands.json
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ fashion\_test.csv
â”‚   â”œâ”€â”€ fashion\_train.csv
â”‚   â”œâ”€â”€ mnist\_test.csv
â”‚   â””â”€â”€ mnist\_train.csv
â”œâ”€â”€ include
â”‚   â”œâ”€â”€ activations
â”‚   â”‚   â”œâ”€â”€ GELU.hpp
â”‚   â”‚   â””â”€â”€ ReLU.hpp
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â””â”€â”€ Tensor.hpp
â”‚   â”œâ”€â”€ layers
â”‚   â”‚   â”œâ”€â”€ Dense.hpp
â”‚   â”‚   â”œâ”€â”€ Embeddings.hpp
â”‚   â”‚   â”œâ”€â”€ FeedForward.hpp
â”‚   â”‚   â”œâ”€â”€ Layer.hpp
â”‚   â”‚   â”œâ”€â”€ LayerNorm.hpp
â”‚   â”‚   â”œâ”€â”€ MultiHeadAttention.hpp
â”‚   â”‚   â””â”€â”€ PatchEmbedding.hpp
â”‚   â”œâ”€â”€ losses
â”‚   â”‚   â”œâ”€â”€ CrossEntropy.hpp
â”‚   â”‚   â””â”€â”€ Loss.hpp
â”‚   â”œâ”€â”€ model
â”‚   â”‚   â”œâ”€â”€ Trainer.hpp
â”‚   â”‚   â”œâ”€â”€ TransformerEncoderBlock.hpp
â”‚   â”‚   â””â”€â”€ VisionTransformer.hpp
â”‚   â”œâ”€â”€ optimizers
â”‚   â”‚   â”œâ”€â”€ Adam.hpp
â”‚   â”‚   â”œâ”€â”€ Optimizer.hpp
â”‚   â”‚   â””â”€â”€ SGD.hpp
â”‚   â””â”€â”€ utils
â”‚   â”‚   â”œâ”€â”€ DataReader.hpp
â”‚   â”‚   â””â”€â”€ ModelUtils.hpp
â”œâ”€â”€ README.md
â”œâ”€â”€ run.sh
â””â”€â”€ src
â”œâ”€â”€ activations
â”‚   â”œâ”€â”€ GELU.cpp
â”‚   â””â”€â”€ ReLU.cpp
â”œâ”€â”€ core
â”‚   â””â”€â”€ Tensor.cpp
â”œâ”€â”€ layers
â”‚   â”œâ”€â”€ Dense.cpp
â”‚   â”œâ”€â”€ Embeddings.cpp
â”‚   â”œâ”€â”€ FeedForward.cpp
â”‚   â”œâ”€â”€ LayerNorm.cpp
â”‚   â”œâ”€â”€ MultiHeadAttention.cpp
â”‚   â””â”€â”€ PatchEmbedding.cpp
â”œâ”€â”€ losses
â”‚   â””â”€â”€ CrossEntropy.cpp
â”œâ”€â”€ model
â”‚   â”œâ”€â”€ Trainer.cpp
â”‚   â”œâ”€â”€ TransformerEncoderBlock.cpp
â”‚   â””â”€â”€ VisionTransformer.cpp
â”œâ”€â”€ optimizers
â”‚   â””â”€â”€ Adam.cpp
â””â”€â”€ utils
â”œâ”€â”€ DataReader.cpp
â””â”€â”€ ModelUtils.cpp

```